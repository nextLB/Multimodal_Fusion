
"""
    åŸºäºConvNeXtV2çš„è‡ªç›‘ç£è®­ç»ƒç¨‹åº
"""

import datasets
import log
import utils
import sys
import traceback
import torch
import torch.optim as optim
import gc
import config_parameters
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import os
import models
from feature_maps import MyFeatureMapHook


# è®¾å¤‡æ£€æµ‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# åœ¨è¿™é‡Œå®šä¹‰ä¸€ä¸‹è¦å¯è§†åŒ–çš„ç‰¹å¾å±‚
targetLayers = [
    'encoderBeforeIdentity',
    'encoderAfterIdentity',
    'decoderAfterIdentity'
]


def train_one_epoch(model, optimizer, device, epoch, lrScheduler, scaler, trainLoop, saveFeatureMapsPath):
    count = 0
    totalLoss = 0

    for batchIdx, (originalImage, augmentationImage) in enumerate(trainLoop):
        originalImage = originalImage.to(device)
        augmentationImage = augmentationImage.to(device)


        # TODO: æ³¨å†Œç‰¹å¾å±‚
        if epoch % 10 == 0 and batchIdx % 100 == 0:
            # initial feature hook
            hookHandler = MyFeatureMapHook(model,
                                           outputDir=f"{saveFeatureMapsPath}/epoch_{epoch}_batchIndex_{batchIdx}",
                                           imgIndex=0)
            hookHandler.register_hooks(targetLayers)


        # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        with torch.amp.autocast(device.type if device.type != 'mps' else 'cpu', enabled=(device.type == 'cuda')):
            loss, pred, mask = model(augmentationImage, epoch, batchIdx, saveFeatureMapsPath)


        # TODO: ä¿å­˜ç‰¹å¾å±‚
        if epoch % 10 == 0 and batchIdx % 100 == 0:
            # save feature maps
            hookHandler.save_feature_maps()
            hookHandler.remove_hooks()


        # ä½¿ç”¨scalerè¿›è¡Œåå‘ä¼ æ’­
        scaler.scale(loss).backward()
        totalLoss += loss.item()

        # ä½¿ç”¨scaleræ›´æ–°ä¼˜åŒ–å™¨
        scaler.step(optimizer)
        scaler.update()

        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # æ›´æ–°è¿›åº¦æ¡
        trainLoop.set_postfix(train_loss=loss.item())
        count += 1

    # æ›´æ–°å­¦ä¹ ç‡
    lrScheduler.step()
    totalLoss /= count

    return totalLoss


def main():

    # åˆ›å»ºå„ç»“æœå­˜å‚¨è·¯å¾„
    saveModelPath, saveLogFilePath, saveFeatureMapsPath = utils.create_all_path()
    logger = log.setup_logging(saveLogFilePath)
    try:
        logger.info('å¼€å§‹åŠ è½½æœ¬æ¬¡è®­ç»ƒçš„æ•°æ®é›†')
        # åŠ è½½æ•°æ®é›†
        fullDataLoader = datasets.main()
        logger.info('åŠ è½½æ•°æ®é›†å®Œæ¯•')


        logger.info('å®šä¹‰æœ¬æ¬¡è®­ç»ƒæ‰€ä½¿ç”¨çš„æ¨¡å‹')
        pretrainModel = models.get_model()
        logger.info('æ¨¡å‹å®šä¹‰å®Œæ¯•')
        logger.info(f'æœ¬æ¬¡åŠ è½½çš„æ¨¡å‹æ¶æ„å¦‚ä¸‹: {pretrainModel}')


        logger.info('å®šä¹‰æœ¬æ¬¡è®­ç»ƒæ‰€ç”¨çš„ä¼˜åŒ–å™¨')
        optimizer = optim.AdamW(
            pretrainModel.parameters(),
            lr=config_parameters.LEARNING_RATE,
            weight_decay=config_parameters.WEIGHT_DECAY,
            betas=(0.9, 0.999)      # é€šå¸¸ä¿æŒé»˜è®¤å€¼å³å¯
        )
        logger.info('å®šä¹‰ä¼˜åŒ–å™¨å®Œæ¯•')


        logger.info('å®šä¹‰æœ¬æ¬¡è®­ç»ƒæ‰€ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨')
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=config_parameters.MAX_EPOCHS,  # åŠä¸ªä½™å¼¦å‘¨æœŸçš„é•¿åº¦ï¼Œé€šå¸¸è®¾ä¸ºæ€»epochæ•°
            eta_min=1e-5  # æœ€å°å­¦ä¹ ç‡ï¼Œé€šå¸¸æ˜¯åˆå§‹å­¦ä¹ ç‡çš„ 1/1000 æˆ– 1/100
        )
        logger.info('å­¦ä¹ ç‡è°ƒåº¦å™¨å®šä¹‰å®Œæ¯•')


        logger.info('å®šä¹‰æ¢¯åº¦è£å‰ªå™¨')
        scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))
        logger.info('å®šä¹‰æ¢¯åº¦è£å‰ªå™¨å®Œæ¯•')

        # ======================================================== #
        # å¼€å§‹è®­ç»ƒ
        trainLosses = []
        valLosses = []
        valIous = []
        bestTrainLoss = float('inf')
        for epoch in range(config_parameters.MAX_EPOCHS):
            print(f"\nEpoch {epoch + 1}/{config_parameters.MAX_EPOCHS}")
            trainLoop = tqdm(fullDataLoader, desc="training")
            pretrainModel.train()

            trainLoss = train_one_epoch(pretrainModel,
                            optimizer,
                            device,
                            epoch,
                            scheduler,
                            scaler,
                            trainLoop,
                            saveFeatureMapsPath)


            # ç¾åŒ–æŸå¤±è¾“å‡º
            print("\n" + "=" * 60)
            print(f"ğŸ“Š Epoch {epoch + 1}/{config_parameters.MAX_EPOCHS} - Training Results:")
            print(f"   â¤ Average Loss: {trainLoss:.6f}")
            print(f"   â¤ Best Loss So Far: {bestTrainLoss:.6f}")



            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if trainLoss < bestTrainLoss:
                bestTrainLoss = trainLoss
                torch.save(pretrainModel.state_dict(), os.path.join(saveModelPath, "deepLabV3_low_loss.pth"))
                print(f"æ¨¡å‹å·²ä¿å­˜è‡³ {saveModelPath}")


            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            utils.draw_train_picture(trainLosses, valLosses, valIous, saveModelPath)


        del pretrainModel
        gc.collect()
        torch.cuda.empty_cache()



    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
        logger.error(traceback.format_exc())
        sys.exit(1)



if __name__ == '__main__':
    main()







